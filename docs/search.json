[
  {
    "objectID": "Maps.html#static-maps",
    "href": "Maps.html#static-maps",
    "title": "MAPS",
    "section": "1.0 STATIC MAPS",
    "text": "1.0 STATIC MAPS\n\nArctic Sea Ice decline\n\n\n\n\nReport Maps\n Link to Report publication\n\n\n\nPapau New Guinea\n\n\n\nContext: Administrative Map  Tools: ArcGIS Pro Date: January 2023\n\n\n\n\n\nSouth Sudan\n\n\n\nContext: Administrative Map\n\n\n\n\n\n\n## 2.0 INTERACTIVE MAPS\n\n\n\nArGIS Online\nA Bangladesh map showing Spatial flood risk assessment in the Ganges-Brahmaputra-Meghna delta in Bangladesh\nFlood Risk App\nThis map shows Spatial flood risk assessment in the Ganges-Brahmaputra-Meghna delta in Bangladesh. This assessment calculated flood risk using the equation; FLOOD RISK = FLOOD HAZARD x FLOOD EXPOSURE x VULNERABILITIES.(Tutorial Purpose)\n\n\nThe index score used ranges from 0-1. with 0 being least at risk and 1 being most at risk of floods\n\nSomalia River Basin The River Basins in Somalia\n\niFrames are not supported on this page.\n\n\n\nWest African cities (in Progress)\nA Leaflet map to show the location of cities and their nearest cities, boarders and populations\n\n# Load the necessary packages\nlibrary(leaflet)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n# Create a data frame with the capital cities and their coordinates\nwest_africa &lt;- data.frame(\n  country = c(\"Nigeria\", \"Ghana\", \"Sierra Leone\", \"Liberia\", \"Cote d'Ivoire\", \"Burkina Faso\", \"Mali\", \"Senegal\", \"Guinea-Bissau\", \"Guinea\", \"Gambia\", \"Togo\", \"Benin\"),\n  capital_city = c(\"Abuja\", \"Accra\", \"Freetown\", \"Monrovia\", \"Yamoussoukro\", \"Ouagadougou\", \"Bamako\", \"Dakar\", \"Bissau\", \"Conakry\", \"Banjul\", \"Lome\", \"Porto-Novo\"),\n  latitude = c(9.0765, 5.6037, 8.4840, 6.3106, 6.8206, 12.3714, 12.6392, 14.7167, 11.8630, 9.5357, 13.4531, 6.1319, 6.4968),\n  longitude = c(7.3986, -0.1870, -13.2299, -10.8047, -5.2764, -1.5330, -8.0029, -17.4677, -15.5976, -13.6788, -16.5790, 1.2221, 2.6059)\n)\n\n# Create the map using Leaflet\nleaflet(west_africa, options = leafletOptions(width = \"800px\")) %&gt;%\n  addTiles() %&gt;%\n  addMarkers(\n    ~longitude,\n    ~latitude,\n    popup = ~paste(country, \"&lt;br&gt;\", capital_city, sep = \"\")\n  )"
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Quarto Testing",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org.\n\n\n\n\nThis is the content I am working with now\n\n\nI want to also stlye this one as well\n\n\n\nHere is a warning.\n\nMore content.\n\n\nHere is a warning."
  },
  {
    "objectID": "talks.html#quarto",
    "href": "talks.html#quarto",
    "title": "Quarto Testing",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org.\n\n\n\n\nThis is the content I am working with now\n\n\nI want to also stlye this one as well\n\n\n\nHere is a warning.\n\nMore content.\n\n\nHere is a warning."
  },
  {
    "objectID": "talks.html#another-one",
    "href": "talks.html#another-one",
    "title": "Quarto Testing",
    "section": "Another one",
    "text": "Another one\n\n&lt;h1 style=\"text-align: center; font-size: 3em;\"&gt;Portfolio&lt;/h1&gt;\n&lt;p style=\"font-size: 1.2em;\"&gt;\n    Whether you’re looking for &lt;span style=\"background-color: #1D3518;\"&gt;crystal clear communication&lt;/span&gt; of data insights, an \n    &lt;span style=\"background-color: #ccffcc;\"&gt;artistic response&lt;/span&gt; to the data, or something between the two, I enjoy putting together \n    stand-out visuals which will keep the conversation going. From academic graphs to artistic commemorations, \n    via interactive visualisations, generative art and animations with a sense of humour, my portfolio features \n    a range of different styles, all coded straight from the data. \n    &lt;br&gt;&lt;br&gt;\n    Click and scroll for full screen view. I have also included here some packages and Shiny Apps I have built. \n    &lt;strong&gt;Happy browsing!&lt;/strong&gt;\n&lt;/p&gt;"
  },
  {
    "objectID": "talks.html#imagies",
    "href": "talks.html#imagies",
    "title": "Quarto Testing",
    "section": "Imagies",
    "text": "Imagies\n\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\n\nAn Elephant\n\n\nThis is illustrated well by Figure 1."
  },
  {
    "objectID": "talks.html#tables",
    "href": "talks.html#tables",
    "title": "Quarto Testing",
    "section": "Tables",
    "text": "Tables\n\n\n\nDefault\nLeft\nRight\nCenter\n\n\n\n\n12\n12\n12\n12\n\n\n123\n123\n123\n123\n\n\n1\n1\n1\n1\n\n\n\n\nFruit prices\n\n\n\n\n\n\nfruit\nprice\n\n\n\n\napple\n2.05\n\n\npear\n1.37\n\n\norange\n3.09"
  },
  {
    "objectID": "talks.html#embedding-from-other-documents",
    "href": "talks.html#embedding-from-other-documents",
    "title": "Quarto Testing",
    "section": "Embedding from Other Documents",
    "text": "Embedding from Other Documents\nHere is the embebded document"
  },
  {
    "objectID": "talks.html#sources-of-inspiration",
    "href": "talks.html#sources-of-inspiration",
    "title": "Quarto Testing",
    "section": "Sources of inspiration",
    "text": "Sources of inspiration\n\n\n#TidyTuesday on Twitter - get involved!\nDataviz / design books\nKids books (palettes!)\nDatajournalists (John Burn-Murdoch - @jburnmurdoch)\nArtists (local art gallery / Twitter accounts e.g. @womensart1)"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Attempting drivers of migration",
    "section": "",
    "text": "Migration patterns are a complex interplay influenced by a multitude of factors. This assessment delves into the case of Mexico, a country with a long history of emigration, exploring how persistent macro-level trends might fuel future migration flows. Mexico’s position as a major sending country, with a significant number of emigrants compared to immigrants, makes it a compelling case study for analyzing potential drivers of migration.\nData from the World Bank indicates Mexico has the highest number of emigrants (11.22 million) in the world, while immigration remains lower (1.2 million) World Bank - Population Review. Net migration figures further support this trend, with Mexico experiencing a net loss of -51,399 migrants in 2022 and an average negative net migration of -223,830 over the past 63 years World Bank.\nThe International Organization for Migration’s (IOM) framework guides this analysis, examining drivers of international migration at Macro, Meso, and Micro levels. While migration is likely to increase if current trends persist, this assessment focuses on Macro-level variables due to easier data access and modeling. More so allows us to build a strong foundation for analysis. This approach allows for future upscaling to continental or state levels. Analyzing Micro and Meso levels is recommended for in-depth understanding of country-specific nuances.\n\n\n\n\n\n\n- General Caveat\n\n\n\n\n\n\nIt’s important to acknowledge that this analysis focuses on macro-level variables. While these factors provide valuable insights, they may not capture the full complexity of individual migration decisions. Micro and meso factors, such as individual circumstances, family networks, and local community dynamics, can also significantly influence people’s choices to migrate.\nIt’s important to recognize that machine learning models can inherit biases from the data used for training. The results presented here should be interpreted cautiously, acknowledging the potential for biases that might overestimate or underestimate the impact of certain factors.\n\n\n\n\n\nData were gathered from the following links and sources, incorporating specific variables relevant to the study:\n\nDependent Variables:\n\nNet Migration: The difference between the number of people entering and leaving a geographic area over a specific period, serving as a proxy for population movement. World Bank - Net Migration\n\nIndependent Variables:\n\nEconomic Factors:\n\nEmployment Rate: Reflects the availability of jobs and the health of the economic environment. Regions with higher employment rates attract people due to better job opportunities, acting as a push or pull factor for migrants. This variable is closely linked to Unemployment rates, where lower unemployment rates indicate higher employment opportunities. World Bank - Unemployment\nIncome Levels: The average income or economic prosperity in a region, influencing migration as higher income levels often correlate with a higher standard of living. World Bank - Income Level\n\nSocial and Demographic Factors:\n\nMortality Rate: While not a direct predictor of migration, it can influence the desirability of a location indirectly through its impact on life expectancy and the quality of healthcare services. World Bank - Mortality Rate\nPopulation Density: Provides insight into how densely populated a region is, which can impact migrants’ decisions. World Bank - Population Density\nPolitical Instability: An indicator of the stability of governance in a region, which can significantly affect migration patterns. World Bank - Worldwide Governance Indicators\n\n\n\nModel Equation\n\n\nNetMigration = \\(\\beta_0\\) + \\(\\beta_1\\) x Political Instability + \\(\\beta_2\\) x Mortiality rate (under 5) + \\(\\beta_3\\)x unemployment + \\(\\beta_4\\) x Government expenditure onEducation + \\(\\beta_5\\) x GNI Income* x e\n\n\n\\(\\beta_0\\) as intercept, \\(\\beta_1\\) , \\(\\beta_2\\) , \\(\\beta_3\\) , \\(\\beta_4\\) , and \\(\\beta_5\\) as coefficients\nThe model will try to understand what affects net migration (the difference between the number of people entering and leaving a country) by looking at several factors: political instability, mortality rate for children under 5 years old, unemployment rate, government spending on education, and Gross National Income (GNI) per capital."
  },
  {
    "objectID": "posts/post-with-code/index.html#load-necessary-libraries",
    "href": "posts/post-with-code/index.html#load-necessary-libraries",
    "title": "Attempting drivers of migration",
    "section": "",
    "text": "Migration patterns are a complex interplay influenced by a multitude of factors. This assessment delves into the case of Mexico, a country with a long history of emigration, exploring how persistent macro-level trends might fuel future migration flows. Mexico’s position as a major sending country, with a significant number of emigrants compared to immigrants, makes it a compelling case study for analyzing potential drivers of migration.\nData from the World Bank indicates Mexico has the highest number of emigrants (11.22 million) in the world, while immigration remains lower (1.2 million) World Bank - Population Review. Net migration figures further support this trend, with Mexico experiencing a net loss of -51,399 migrants in 2022 and an average negative net migration of -223,830 over the past 63 years World Bank.\nThe International Organization for Migration’s (IOM) framework guides this analysis, examining drivers of international migration at Macro, Meso, and Micro levels. While migration is likely to increase if current trends persist, this assessment focuses on Macro-level variables due to easier data access and modeling. More so allows us to build a strong foundation for analysis. This approach allows for future upscaling to continental or state levels. Analyzing Micro and Meso levels is recommended for in-depth understanding of country-specific nuances.\n\n\n\n\n\n\n- General Caveat\n\n\n\n\n\n\nIt’s important to acknowledge that this analysis focuses on macro-level variables. While these factors provide valuable insights, they may not capture the full complexity of individual migration decisions. Micro and meso factors, such as individual circumstances, family networks, and local community dynamics, can also significantly influence people’s choices to migrate.\nIt’s important to recognize that machine learning models can inherit biases from the data used for training. The results presented here should be interpreted cautiously, acknowledging the potential for biases that might overestimate or underestimate the impact of certain factors.\n\n\n\n\n\nData were gathered from the following links and sources, incorporating specific variables relevant to the study:\n\nDependent Variables:\n\nNet Migration: The difference between the number of people entering and leaving a geographic area over a specific period, serving as a proxy for population movement. World Bank - Net Migration\n\nIndependent Variables:\n\nEconomic Factors:\n\nEmployment Rate: Reflects the availability of jobs and the health of the economic environment. Regions with higher employment rates attract people due to better job opportunities, acting as a push or pull factor for migrants. This variable is closely linked to Unemployment rates, where lower unemployment rates indicate higher employment opportunities. World Bank - Unemployment\nIncome Levels: The average income or economic prosperity in a region, influencing migration as higher income levels often correlate with a higher standard of living. World Bank - Income Level\n\nSocial and Demographic Factors:\n\nMortality Rate: While not a direct predictor of migration, it can influence the desirability of a location indirectly through its impact on life expectancy and the quality of healthcare services. World Bank - Mortality Rate\nPopulation Density: Provides insight into how densely populated a region is, which can impact migrants’ decisions. World Bank - Population Density\nPolitical Instability: An indicator of the stability of governance in a region, which can significantly affect migration patterns. World Bank - Worldwide Governance Indicators\n\n\n\nModel Equation\n\n\nNetMigration = \\(\\beta_0\\) + \\(\\beta_1\\) x Political Instability + \\(\\beta_2\\) x Mortiality rate (under 5) + \\(\\beta_3\\)x unemployment + \\(\\beta_4\\) x Government expenditure onEducation + \\(\\beta_5\\) x GNI Income* x e\n\n\n\\(\\beta_0\\) as intercept, \\(\\beta_1\\) , \\(\\beta_2\\) , \\(\\beta_3\\) , \\(\\beta_4\\) , and \\(\\beta_5\\) as coefficients\nThe model will try to understand what affects net migration (the difference between the number of people entering and leaving a country) by looking at several factors: political instability, mortality rate for children under 5 years old, unemployment rate, government spending on education, and Gross National Income (GNI) per capital."
  },
  {
    "objectID": "posts/post-with-code/index.html#step-1-data-gathering",
    "href": "posts/post-with-code/index.html#step-1-data-gathering",
    "title": "Attempting drivers of migration",
    "section": "Step 1: Data Gathering",
    "text": "Step 1: Data Gathering\nLoad the variables\n\n# Load first set of dataset  -------------------------\n# Define the base directory\nbase_dir &lt;- \"C:/Users/orenaike/OneDrive/02_JOBS/OT_CV/Jobs_Application/IOM/Interview_Prep/Prep_Grace\"\n\n# Variables\n# Net Migration ~  pol_instab + pop_density + under_5_mortality_rate + total_unemployment + govt_expenditure_education + gni_income\n\n# Define subdirectories\nsub_dirs &lt;- c(\"API_EN.POP.DNST_DS2_en_csv_v2_1512_Pop_Density\",\n              \"API_SH.DYN.MORT_DS2_en_csv_v2_1984_mortality\",\n              \"API_NY.GNP.PCAP.KD_DS2_en_csv_v2_3157_GNI\",\n              \"API_SL.UEM.TOTL.NE.ZS_DS2_en_csv_v2_Unemployment\",\n              \"API_SE.XPD.TOTL.GD.ZS_DS2_en_csv_v2_14_govt_exp\",\n              \"API_SM.POP.NETM_DS2_en_csv_v2_105_Net migration\")\n\n# Initialize an empty list to store data frames\ndf_list &lt;- list()\n\n# Loop through each subdirectory\nfor(dir in sub_dirs) {\n  # Construct the full path to the directory\n  full_dir &lt;- file.path(base_dir, dir)\n  \n  # Find the CSV file that starts with \"API\" in the directory\n  # csv_file &lt;- list.files(full_dir, pattern = \"^API.*//.csv$\", full.names = TRUE)\n  csv_file &lt;- list.files(full_dir, pattern = \"^API.*\\\\.csv$\", full.names = TRUE)\n  \n  # Check if exactly one file is found\n  if(length(csv_file) == 1) {\n    # Read the CSV file from row 5, skip the first 4 rows\n    data &lt;- read.csv(csv_file, skip = 4, stringsAsFactors = FALSE)\n    \n    # Add the data frame to the list\n    df_list[[dir]] &lt;- data\n  } else {\n    cat(\"No file or multiple files found in\", dir, \"/n\")\n  }\n}\n\n# Concatenate all data frames in the list into one\nall_data &lt;- bind_rows(df_list) # The data is currently in wide format. We need to cover to Long for easy analysis \n\n# Check the structure of the concatenated data frame\ndim(all_data) \n\n[1] 1596   68\n\n# load political instability data ----------------\n\npolitical_instab &lt;- read_csv(\"C:/Users/orenaike/OneDrive/02_JOBS/OT_CV/Jobs_Application/IOM/Interview_Prep/Prep_Grace/P_Data_Extract_From_Worldwide_Governance_Indicators/0ac43a1d-ff11-4838-93a9-738e096ed561_Data.csv\")\n\nRows: 219 Columns: 15\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (15): Series Name, Series Code, Country Name, Country Code, 2000 [YR2000...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\npolitical_instab_v1 &lt;- political_instab |&gt;\n  janitor::clean_names() |&gt;\n  dplyr::filter(series_name == \"Political Stability and Absence of Violence/Terrorism: Estimate\")\n\nhead(political_instab_v1) #check the first few\n\n# A tibble: 6 × 15\n  series_name    series_code country_name country_code x2000_yr2000 x2013_yr2013\n  &lt;chr&gt;          &lt;chr&gt;       &lt;chr&gt;        &lt;chr&gt;        &lt;chr&gt;        &lt;chr&gt;       \n1 Political Sta… PV.EST      Afghanistan  AFG          -2.43896889… -2.51934909…\n2 Political Sta… PV.EST      Albania      ALB          -0.53998959… 0.091929785…\n3 Political Sta… PV.EST      Algeria      DZA          -1.43257737… -1.20237147…\n4 Political Sta… PV.EST      American Sa… ASM          ..           0.928985774…\n5 Political Sta… PV.EST      Andorra      AND          1.166981458… 1.283926010…\n6 Political Sta… PV.EST      Angola       AGO          -2.03817391… -0.39123347…\n# ℹ 9 more variables: x2014_yr2014 &lt;chr&gt;, x2015_yr2015 &lt;chr&gt;,\n#   x2016_yr2016 &lt;chr&gt;, x2017_yr2017 &lt;chr&gt;, x2018_yr2018 &lt;chr&gt;,\n#   x2019_yr2019 &lt;chr&gt;, x2020_yr2020 &lt;chr&gt;, x2021_yr2021 &lt;chr&gt;,\n#   x2022_yr2022 &lt;chr&gt;"
  },
  {
    "objectID": "posts/New_post/index.html",
    "href": "posts/New_post/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2\n\n\nIn a previous post, we talked about the Oscars and whether prestige awards still matter. Let’s turn now to what is perhaps the mother of all prestige awards — the Nobel Prize. The legacy of Swedish industrialist Alfred Nobel, it honors outstanding achievement in the fields of chemistry, literature, medicine, peace, physics, and, since 1969, economics. Winners — ahem, laureates — receive unparalleled stature not just in their fields but in the public sphere (deservedly or otherwise).\nIn its 120-year history, close to a thousand individuals (and some two dozen organizations) have been given a Nobel. What are they like? As an elite group, you can probably guess that they would tend towards oldness, whiteness, and maleness. But how old, how white, and how male? Fortunately, NobelPrize.org has an API for downloading data on all laureates, through which I was able to compile, for each laureate: the year and category they won in, their sex, their birth date, and their birth country (using modern borders). I won’t be including organizations in my analysis.\nLet’s take a look at the cleaned dataset using reactable, a wonderful table-making package by Greg Lin that I sure wish I discovered earlier. Below is a sortable, searchable, paginated table of the complete dataset:"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Orenaike",
    "section": "",
    "text": "Attempting drivers of migration\n\n\nMexico on the Move: How Macro Forces Drive Migration\n\n\n\nR\n\n\nMachine Learning\n\n\nMigration\n\n\n\n\n\n\n\n\n\nMar 14, 2024\n\n\nOluwatosin Orenaike\n\n\n\n\n\n\n\n\n\n\n\n\nDemonstrating Predictive Modeling for Site Selection: A Proof of Concept\n\n\nLeverage machine learning to make data-driven decisions about business locations using R and Random Forest modeling\n\n\n\nR\n\n\nKobotoolbox\n\n\nData Collection\n\n\n\n\n\n\n\n\n\nJan 5, 2024\n\n\nOluwatosin Orenaike\n\n\n\n\n\n\n\n\n\n\n\n\nGeofencing with R for Enhanced Data Collection in Kobotoolbox\n\n\nGeofencing is a powerful tool that combines location technology with data analysis allowing you to automate and check data collection within predefined geographical boundaries.\n\n\n\nR\n\n\nKobotoolbox\n\n\nData Collection\n\n\n\n\n\n\n\n\n\nJan 5, 2024\n\n\nOluwatosin Orenaike\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Knowledge Sharing",
    "section": "",
    "text": "I’m a passionate data scientist with expertise in leveraging geospatial technologies to tackle environmental challenges, enhance human security, and support safe migration in humanitarian efforts. At the Global Data Institute(GDI) specifically, the Analytics, Knowledge and Output (AKO) unit I provide operatonal support for country missions with data collection tools, data-driven analysis and insights, the creation of informative maps, and training & capacity building programs. My experience spans,Data Management & Analysis. GIS & Remote Sensing: and Information Management: \n Thanks for stopping by! If you have any questions or would like to work together, please don’t hesitate to contact me.\n \n  \n   \n  \n    \n     twitter\n  \n  \n    \n     Github\n  \n  \n    \n     linkedin\n  \n  \n    \n     Goodreads"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Orenaikeblog",
    "section": "",
    "text": "University of California, San Diego | San Diego, CA PhD in Mathematics | Sept 2011 - June 2015\nMacalester College | St. Paul MA B.A in Economics | Sept 2007 - June 2011"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "Orenaikeblog",
    "section": "",
    "text": "Wengo Analytics | Head Data Scientist | April 2018 - present\nGeoScynce | Chief Analyst | Sept 2012 - April 2018  Thanks for stopping by! If you have any questions or would like to work together, please don’t hesitate to contact me."
  },
  {
    "objectID": "portfolio.html#welcome-to-my-page",
    "href": "portfolio.html#welcome-to-my-page",
    "title": "Knowledge Sharing",
    "section": "Welcome to my page!",
    "text": "Welcome to my page!\nHere, you will find samples of my work. I specialize in using R, Python, and Adobe Creative Suite, including InDesign, Photoshop, and Illustrator, for data visualization, data wrangling, and data analysis. I also have experience with ArcGIS Pro, ArcGIS Online, and QGIS for geospatial data analysis, as well as Jupyter and R Markdown for data visualization and geospatial analysis. I can use SQL in managing relational database. Additionally, I am currently learning Web Development.\n\nMy core skills include:\n\nStrong computer background with experience in relational databases, Microsoft applications, spreadsheets, word processing, GIS, graphic design, and desktop publishing.\nData management, research, and analysis experience with both quantitative and qualitative data.\nExperience with analytics and visualization tools such as open viz libraries like Shiny and flexdashboard.\nDemonstrated experience developing risk index model at national and global scale\nProficient in using and maintaining spatial and non-spatial technology products.\nStrong information management architecture knowledge, Kobo development, concepts of Relational Database Systems.\nExperience working with local and international organizations, including UN-IOM, UNDP, UNSPIDER and EU.\nExcellent analytical and organizational skills, communication, writing and reporting skills, and multicultural skills.\nExpertise in Earth Observation and GIS Risk Analysis and remote sensing analysis (Floods extraction, buildings damage assessment, population affected monitoring).\nAbility to build strong and sustainable relationships, interact at all levels within the organization, work independently, and lead teams.\nThematic experience includes Migration, Displacement, Humanitarian and Droughts.I am also exploring application of AI to optimize outputs.\n\n\n\n\n Thanks for stopping by! If you have any questions or would like to work together, please don’t hesitate to contact me."
  },
  {
    "objectID": "posts/geofencing/index.html",
    "href": "posts/geofencing/index.html",
    "title": "Geofencing with R for Enhanced Data Collection in Kobotoolbox",
    "section": "",
    "text": "Geofencing is a powerful tool that combines location technology with data analysis, allows you to automate data collection within predefined geographical boundaries. I will show you a step-by-step guide, using R, to integrate geofencing with Kobotoolbox and boost your data collection efficiency. This process involves two major steps:\n\nGetting the vertex for your location of interest.\nUploading vertex to KoboToolbox (link to a video).\n\nImagine this:\n\nYou enter a designated study area, and data collection triggers automatically. No more remembering to press buttons or check locations!\nYour data comes from precisely where you need it, thanks to the power of geofencing boundaries. No more worrying about stray data points or missed locations!\nKobotoolbox forms adapt based on location, with possibilities of dynamically changing questions or displaying relevant information for specific areas.\n\nLet’s dive in!"
  },
  {
    "objectID": "posts/geofencing/index.html#step-1-shape-up-your-boundaries",
    "href": "posts/geofencing/index.html#step-1-shape-up-your-boundaries",
    "title": "Geofencing with R for Enhanced Data Collection in Kobotoolbox",
    "section": "Step 1: Shape Up Your Boundaries",
    "text": "Step 1: Shape Up Your Boundaries\nFirst, define your geofencing areas. Grab the shapefiles for your target zones — these handy files store information about geographical boundaries. Our example uses a sample shapefile named “Sample_locations” with details like Sites, Zones, House, and Blocks. Download it here: [link to your sample shapefile]\n\n# Load your shapefile (ensure you provide the correct path)\nSample_locations &lt;- st_read(\"./data/shp_file/Sample_locations.shp\")\n\nReading layer `Sample_locations' from data source \n  `C:\\Users\\orenaike\\OneDrive\\01_ORENAIKE\\02_CAREER_AND_DEVELOPMENTS\\01_Schools\\Web_Development\\Portfolio\\otomisin.github.io\\posts\\geofencing\\data\\shp_file\\Sample_locations.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 6 features and 4 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 605820.6 ymin: 853074.4 xmax: 606177 ymax: 853555.9\nProjected CRS: WGS 84 / UTM zone 35N\n\n# Check and plot the data table to understand its structure\nprint(st_drop_geometry(Sample_locations))\n\n  OBJECTID Zone_ Block   House\n1        1     B     1 House A\n2        2     B     2 House B\n3        3     A     1 House C\n4        4     A     2 House D\n5        5     A     3 House E\n6        6     A     4 House F\n\n# Plot the locations\nSample_locations |&gt;\n  ggplot() +\n  geom_sf() +\n  geom_label(aes(x = st_coordinates(st_centroid(geometry))[, 1], \n                 y = st_coordinates(st_centroid(geometry))[, 2], \n                 label = House), \n             size = 3, fill = \"lightblue\", color = \"black\") +\n  theme_void()\n\n\n\n\n\n\n\n\n."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/geofencing/index.html#step-2-extract-those-coordinates",
    "href": "posts/geofencing/index.html#step-2-extract-those-coordinates",
    "title": "Geofencing with R for Enhanced Data Collection in Kobotoolbox",
    "section": "Step 2: Extract Those Coordinates",
    "text": "Step 2: Extract Those Coordinates\nNow, use R’s st_geometry function to extract the precise latitude and longitude values from your shapefile. We’ll store these coordinates in a separate data frame for easier manipulation.\n\n# Extract coordinates using the same Coordinate Reference System (CRS) as the original data\npolygon_vertices &lt;- lapply(st_geometry(Sample_locations), st_coordinates)\n\n# Map each polygon_vertices list to its OBJECTID row\nmapped_data_v1 &lt;- Map(function(vertices, objectid) {\n  data.frame(OBJECTID = objectid,\n             p_longitude = vertices[, \"X\"],\n             p_latitude = vertices[, \"Y\"])\n}, polygon_vertices, Sample_locations$OBJECTID)\n\n# Combine the mapped data frames into a single data frame\nmapped_data_v1 &lt;- do.call(rbind, mapped_data_v1)\n\n# Integrate the mapped data with the original shapefile data\nSample_locations_points &lt;- Sample_locations |&gt;\n  as.data.frame() |&gt;\n  left_join(mapped_data_v1, by = \"OBJECTID\") |&gt;\n  st_as_sf(coords = c(\"p_longitude\", \"p_latitude\"), crs = st_crs(Sample_locations))\n\n\nsummary(Sample_locations_points)\n\n    OBJECTID        Zone_              Block              House          \n Min.   :1.000   Length:66          Length:66          Length:66         \n 1st Qu.:1.000   Class :character   Class :character   Class :character  \n Median :2.000   Mode  :character   Mode  :character   Mode  :character  \n Mean   :2.667                                                           \n 3rd Qu.:4.000                                                           \n Max.   :6.000                                                           \n          geometry \n POINT        :66  \n epsg:32635   : 0  \n +proj=utm ...: 0"
  },
  {
    "objectID": "posts/geofencing/index.html#step-3-generate-your-id-nodes",
    "href": "posts/geofencing/index.html#step-3-generate-your-id-nodes",
    "title": "Geofencing with R for Enhanced Data Collection in Kobotoolbox",
    "section": "Step 3: Generate Your ID Nodes",
    "text": "Step 3: Generate Your ID Nodes\n\nSample_locations_points_v1 &lt;- Sample_locations_points %&gt;%\n  group_by(Block) %&gt;%\n  mutate(id_node = paste0(Block, \"_\", row_number())) %&gt;%\n  ungroup()\n\n# View the points and polygons on a map\nggplot() +\n  geom_sf(data = Sample_locations) +\n  geom_label(data = Sample_locations_points_v1,\n             aes(x = st_coordinates(st_centroid(geometry))[, 1],\n                 y = st_coordinates(st_centroid(geometry))[, 2],\n                 label = House),\n             size = 3, fill = \"lightblue\", color = \"black\") +\n  geom_sf(data = Sample_locations_points_v1) +\n  theme_void()"
  },
  {
    "objectID": "posts/geofencing/index.html#step-5-export-the-spatial-data-for-kobotoolbox-integration",
    "href": "posts/geofencing/index.html#step-5-export-the-spatial-data-for-kobotoolbox-integration",
    "title": "Geofencing with R for Enhanced Data Collection in Kobotoolbox",
    "section": "Step 5: Export the Spatial Data for Kobotoolbox Integration",
    "text": "Step 5: Export the Spatial Data for Kobotoolbox Integration\n\nwrite.csv(Sample_locations_points_v1, \"./Sample_locations_points_v1.csv\")\n\n# The exported CSV file can now be used in Kobotoolbox for enhanced data collection with geofencing\n\nStep 5: Integrate with Kobotoolbox\nCongratulations! You now have a spatial data frame enriched with precise coordinates and unique identifiers, ready to be seamlessly integrated with Kobotoolbox. Unleash the power of location-aware data collection, with automatic form triggers and data collection tailored to specific geographical zones.\nBeyond the Code: Here are some additional tips to take your geofencing journey to the next level:\n\nReal-world examples: Think about using geofencing to study air quality in specific city districts, automatically triggering data collection at designated times.\nChallenges and solutions: Consider potential challenges like battery drain on data collectors and GPS limitations. Optimize data collection forms and schedule strategically to mitigate these.\nDive deeper: Explore our GitHub repository (link here) for the full code buffet and detailed tutorials to become a geofencing master.\n\nDon’t let your data collection be stuck in the manual age! Leverage the magic of geofencing with R and Kobotoolbox to supercharge your fieldwork efficiency and precision and experience the power of automation, precision, and streamlined workflows!\nYou can check how to integrate the output into Kobotoolbox. Check this video on how to."
  },
  {
    "objectID": "Mapv1.html#static-maps",
    "href": "Mapv1.html#static-maps",
    "title": "MAPS",
    "section": "",
    "text": "Link to Report publication\n\n\n\n\n\n\n\nContext: Administrative Map  Tools: ArcGIS Pro Date: January 2023\n\n\n\n\n\n\n\n\n\nContext: Administrative Map\n\n\n\n\n\n\n## 2.0 INTERACTIVE MAPS\n\n\n\n\nA Bangladesh map showing Spatial flood risk assessment in the Ganges-Brahmaputra-Meghna delta in Bangladesh\nFlood Risk App\nThis map shows Spatial flood risk assessment in the Ganges-Brahmaputra-Meghna delta in Bangladesh. This assessment calculated flood risk using the equation; FLOOD RISK = FLOOD HAZARD x FLOOD EXPOSURE x VULNERABILITIES.(Tutorial Purpose)\n\n\nThe index score used ranges from 0-1. with 0 being least at risk and 1 being most at risk of floods\n\nSomalia River Basin The River Basins in Somalia\n\n&lt;p&gt;iFrames are not supported on this page.&lt;/p&gt;\n\n\n\n\nA Leaflet map to show the location of cities and their nearest cities, boarders and populations\n\n# Load the necessary packages\nlibrary(leaflet)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n# Create a data frame with the capital cities and their coordinates\nwest_africa &lt;- data.frame(\n  country = c(\"Nigeria\", \"Ghana\", \"Sierra Leone\", \"Liberia\", \"Cote d'Ivoire\", \"Burkina Faso\", \"Mali\", \"Senegal\", \"Guinea-Bissau\", \"Guinea\", \"Gambia\", \"Togo\", \"Benin\"),\n  capital_city = c(\"Abuja\", \"Accra\", \"Freetown\", \"Monrovia\", \"Yamoussoukro\", \"Ouagadougou\", \"Bamako\", \"Dakar\", \"Bissau\", \"Conakry\", \"Banjul\", \"Lome\", \"Porto-Novo\"),\n  latitude = c(9.0765, 5.6037, 8.4840, 6.3106, 6.8206, 12.3714, 12.6392, 14.7167, 11.8630, 9.5357, 13.4531, 6.1319, 6.4968),\n  longitude = c(7.3986, -0.1870, -13.2299, -10.8047, -5.2764, -1.5330, -8.0029, -17.4677, -15.5976, -13.6788, -16.5790, 1.2221, 2.6059)\n)\n\n# Create the map using Leaflet\nleaflet(west_africa, options = leafletOptions(width = \"800px\")) %&gt;%\n  addTiles() %&gt;%\n  addMarkers(\n    ~longitude,\n    ~latitude,\n    popup = ~paste(country, \"&lt;br&gt;\", capital_city, sep = \"\")\n  )"
  },
  {
    "objectID": "Mapv1.html",
    "href": "Mapv1.html",
    "title": "MAPS",
    "section": "",
    "text": "Link to Report publication\n\n\n\n\n\n\n\nContext: Administrative Map  Tools: ArcGIS Pro Date: January 2023\n\n\n\n\n\n\n\n\n\nContext: Administrative Map\n\n\n\n\n\n\n## 2.0 INTERACTIVE MAPS\n\n\n\n\nA Bangladesh map showing Spatial flood risk assessment in the Ganges-Brahmaputra-Meghna delta in Bangladesh\nFlood Risk App\nThis map shows Spatial flood risk assessment in the Ganges-Brahmaputra-Meghna delta in Bangladesh. This assessment calculated flood risk using the equation; FLOOD RISK = FLOOD HAZARD x FLOOD EXPOSURE x VULNERABILITIES.(Tutorial Purpose)\n\n\nThe index score used ranges from 0-1. with 0 being least at risk and 1 being most at risk of floods\n\nSomalia River Basin The River Basins in Somalia\n\n&lt;p&gt;iFrames are not supported on this page.&lt;/p&gt;\n\n\n\n\nA Leaflet map to show the location of cities and their nearest cities, boarders and populations\n\n# Load the necessary packages\nlibrary(leaflet)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n# Create a data frame with the capital cities and their coordinates\nwest_africa &lt;- data.frame(\n  country = c(\"Nigeria\", \"Ghana\", \"Sierra Leone\", \"Liberia\", \"Cote d'Ivoire\", \"Burkina Faso\", \"Mali\", \"Senegal\", \"Guinea-Bissau\", \"Guinea\", \"Gambia\", \"Togo\", \"Benin\"),\n  capital_city = c(\"Abuja\", \"Accra\", \"Freetown\", \"Monrovia\", \"Yamoussoukro\", \"Ouagadougou\", \"Bamako\", \"Dakar\", \"Bissau\", \"Conakry\", \"Banjul\", \"Lome\", \"Porto-Novo\"),\n  latitude = c(9.0765, 5.6037, 8.4840, 6.3106, 6.8206, 12.3714, 12.6392, 14.7167, 11.8630, 9.5357, 13.4531, 6.1319, 6.4968),\n  longitude = c(7.3986, -0.1870, -13.2299, -10.8047, -5.2764, -1.5330, -8.0029, -17.4677, -15.5976, -13.6788, -16.5790, 1.2221, 2.6059)\n)\n\n# Create the map using Leaflet\nleaflet(west_africa, options = leafletOptions(width = \"800px\")) %&gt;%\n  addTiles() %&gt;%\n  addMarkers(\n    ~longitude,\n    ~latitude,\n    popup = ~paste(country, \"&lt;br&gt;\", capital_city, sep = \"\")\n  )"
  },
  {
    "objectID": "welcome.html#welcome-to-my-page",
    "href": "welcome.html#welcome-to-my-page",
    "title": "Knowledge Sharing",
    "section": "Welcome to my page!",
    "text": "Welcome to my page!\nHere, you’ll find a collection of my work and knowledge sharing focused on data visualization, data wrangling, and data analysis. I leverage a diverse skillset that includes:\n\nProgramming Languages: R, Python and Javascript\nData Visualization & Analysis: Adobe Creative Suite (InDesign, Photoshop, Illustrator), Jupyter Notebooks, R Markdown\nGeospatial Analysis: ArcGIS Pro, ArcGIS Online, QGIS\nDatabase Management: SQL\nWeb Development\n\nData & Geospatial Expertise I’m a passionate data scientist with expertise in leveraging geospatial technologies to tackle environmental challenges, enhance human security, and support safe migration in humanitarian efforts. At the Global Data Institute(GDI) specifically, the Analytics, Knowledge and Output (AKO) unit I provide operatonal support for country missions with data collection tools, data-driven analysis and insights, the creation of informative maps, and training & capacity building programs. My experience spans:\n\nData Management & Analysis: Proficient in managing, researching, and analyzing both quantitative and qualitative data. I have strong experience with relational databases, Microsoft applications, and a suite of data visualization tools including Javascript libraries (Shiny, Streamlit) and commercial platforms (Tableau, Power BI, ObservableHQ).\nGIS & Remote Sensing:  An expert in Earth Observation, GIS analysis, and remote sensing applications like flood extraction, building damage assessment, and population monitoring.\nInformation Management:  Possess a strong understanding of information management architecture, Kobo development, and relational database systems. Highly proficient in using and maintaining both spatial and non-spatial technology products.\n\nAlways Learning & Growing\nBeyond my current skillset, I’m actively exploring cloud computing, machine learning, and artificial intelligence to further enhance and advance my ability to deliver positive social and environmental solutions through data analysis. Kindly reach out if you got an idea!\n\n\n\n Thanks for stopping by! If you have any questions or would like to work together, please don’t hesitate to contact me."
  },
  {
    "objectID": "about.html#welcome-to-my-page",
    "href": "about.html#welcome-to-my-page",
    "title": "About me",
    "section": "Welcome to my page!",
    "text": "Welcome to my page!\nHere, you’ll find a collection of my work and knowledge sharing focused on data visualization, data wrangling, and data analysis. I leverage a diverse skillset that includes:\n\nProgramming Languages: R, Python and Javascript\nData Visualization & Analysis: Adobe Creative Suite (InDesign, Photoshop, Illustrator), Jupyter Notebooks, R Markdown\nGeospatial Analysis: ArcGIS Pro, ArcGIS Online, QGIS\nDatabase Management: SQL\nWeb Development\n\nData & Geospatial Expertise\nI’m a passionate data scientist with expertise in leveraging geospatial technologies to tackle environmental challenges, enhance human security, and support safe migration in humanitarian efforts. At the Global Data Institute(GDI) specifically, the Analytics, Knowledge and Output (AKO) unit I provide operatonal support for country missions with data collection tools, data-driven analysis and insights, the creation of informative maps, and training & capacity building programs. My experience spans:\n\nData Management & Analysis:\n\nProficient in managing, researching, and analyzing both quantitative and qualitative data. I have strong experience with relational databases, Microsoft applications, and a suite of data visualization tools including Javascript libraries (Shiny, Streamlit) and commercial platforms (Tableau, Power BI, ObservableHQ). - GIS & Remote Sensing: \nAn expert in Earth Observation, GIS analysis, and remote sensing applications like flood extraction, building damage assessment, and population monitoring. - Information Management: \nPossess a strong understanding of information management architecture, Kobo development, and relational database systems. Highly proficient in using and maintaining both spatial and non-spatial technology products.\nAlways Learning & Growing\nBeyond my current skillset, I’m actively exploring cloud computing, machine learning, and artificial intelligence to further enhance and advance my ability to deliver positive social and environmental solutions through data analysis. Kindly reach out if you got an idea!\n\n\n\n Thanks for stopping by! If you have any questions or would like to work together, please don’t hesitate to contact me.\n\n\n\n:::"
  },
  {
    "objectID": "posts/Predictive_model/index.html",
    "href": "posts/Predictive_model/index.html",
    "title": "Demonstrating Predictive Modeling for Site Selection: A Proof of Concept",
    "section": "",
    "text": "In today’s competitive business landscape, making informed decisions about business locations can mean the difference between success and failure. While traditional methods rely heavily on intuition and basic market research, modern data science techniques offer a more robust approach. This demonstration shows how machine learning could be applied to business site selection using synthetic data. While the data is simulated, the techniques and approach demonstrate the potential of data-driven decision making in location planning"
  },
  {
    "objectID": "posts/Predictive_model/index.html#introduction-the-power-of-data-driven-decision-making",
    "href": "posts/Predictive_model/index.html#introduction-the-power-of-data-driven-decision-making",
    "title": "Demonstrating Predictive Modeling for Site Selection: A Proof of Concept",
    "section": "",
    "text": "In today’s competitive business landscape, making informed decisions about business locations can mean the difference between success and failure. While traditional methods rely heavily on intuition and basic market research, modern data science techniques offer a more robust approach. This demonstration shows how machine learning could be applied to business site selection using synthetic data. While the data is simulated, the techniques and approach demonstrate the potential of data-driven decision making in location planning"
  },
  {
    "objectID": "posts/Predictive_model/index.html#the-business-challenge",
    "href": "posts/Predictive_model/index.html#the-business-challenge",
    "title": "Demonstrating Predictive Modeling for Site Selection: A Proof of Concept",
    "section": "The Business Challenge",
    "text": "The Business Challenge\nImagine you’re tasked with expanding your business to new locations. You need to answer a crucial question:\n\n“Given a potential location’s characteristics, what is the probability that a new business site will succeed?”\n\nThis isn’t just about finding a good location – it’s about systematically evaluating multiple factors that contribute to business success, including:\n\nPopulation density\nMedian income levels\nCompetition in the area\nTraffic patterns\nParking availability"
  },
  {
    "objectID": "posts/Predictive_model/index.html#building-a-predictive-model",
    "href": "posts/Predictive_model/index.html#building-a-predictive-model",
    "title": "Demonstrating Predictive Modeling for Site Selection: A Proof of Concept",
    "section": "Building a Predictive Model",
    "text": "Building a Predictive Model\nTo showcase the potential of predictive modeling in site selection, we’ve created a synthetic dataset that mirrors the complexities of real-world location decisions. Our demonstration encompasses 1000 simulated locations across Nigeria, each characterized by carefully generated attributes that reflect typical market indicators. The synthetic nature of this data allows us to explore the full potential of our analytical approach while acknowledging that real-world implementation would require actual historical data.\n\nSetting Up Our Environment\nFirst, we’ll load the necessary libraries and prepare our data:\n\n\nData Preparation\nIn a real-world scenario, you’d have historical data about business locations and their outcomes. For this demonstration, we’ll create a synthetic dataset that mimics real-world patterns:\n\n# Load necessary libraries\n# Load Nigeria's boundary as an sf object\nnigeria &lt;- ne_countries(scale = \"medium\", country = \"Nigeria\", returnclass = \"sf\")\n\n# Define Nigeria's latitude and longitude bounds\nnigeria_lat_range &lt;- c(4.2, 13.9)  \nnigeria_lon_range &lt;- c(2.7, 14.6)  \n\n# Set seed for reproducibility\nset.seed(123)\n\n# Generate sample points across Nigeria\nn_locations &lt;- 1000  \n\nsample_data &lt;- data.frame(\n  location_id = 1:n_locations,\n  latitude = runif(n_locations, nigeria_lat_range[1], nigeria_lat_range[2]),    \n  longitude = runif(n_locations, nigeria_lon_range[1], nigeria_lon_range[2]),   \n  population_density = rnorm(n_locations, mean = 5000, sd = 2000),\n  median_income = rnorm(n_locations, mean = 65000, sd = 15000),\n  competition_count = rpois(n_locations, lambda = 3),\n  traffic_score = runif(n_locations, 1, 100),\n  parking_available = sample(c(TRUE, FALSE), n_locations, replace = TRUE),\n  success = sample(c(1, 0), n_locations, prob = c(0.6, 0.4), replace = TRUE)\n)\n\n# Convert to sf object\nsample_sf &lt;- st_as_sf(sample_data, coords = c(\"longitude\", \"latitude\"), crs = 4326)\n\n# Ensure points are within Nigeria (land area)\nsample_sf &lt;- sample_sf[st_within(sample_sf, nigeria, sparse = FALSE), ]\n\n############ Viz Dat #########################\n# Define a diverging color palette (RdYlBu for better contrast)\npal &lt;- colorNumeric(palette = \"RdYlBu\", domain = sample_sf$median_income, reverse = FALSE)\n\n# Create the Leaflet map with improved color variation\nleaflet(sample_sf) |&gt; \n  addProviderTiles(providers$CartoDB.Positron) %&gt;%\n  addCircleMarkers(\n    radius = 4, \n    color = ~pal(median_income),  # Apply diverging color scale\n    fillOpacity = 0.7,\n    popup = ~paste(\"ID:\", location_id, \n                   \"&lt;br&gt;Population Density:\", round(population_density, 2), \n                   \"&lt;br&gt;Median Income:\", round(median_income, 2))\n  ) %&gt;%\n  addLegend(\n    pal = pal, \n    values = sample_sf$median_income, \n    title = \"Median Income\", \n    opacity = 1\n  )\n\n\n\n\n\n\n\nData Preprocessing\nBefore training our model, we need to prepare our data properly:\n\n# Convert boolean to factor\nsample_data$parking_available &lt;- as.factor(sample_data$parking_available)\nsample_data$success &lt;- as.factor(sample_data$success)\n\n# Scale numeric variables\nnumeric_vars &lt;- c(\"population_density\", \"median_income\", \"competition_count\", \"traffic_score\")\npreprocessed_data &lt;- sample_data\npreprocessed_data[numeric_vars] &lt;- scale(sample_data[numeric_vars])\n\n\n\nModel Training and Evaluation\nWe’ll use a Random Forest model, which is excellent for this type of prediction task:\n\n# Split data into training and testing sets\nset.seed(456)\ntrain_index &lt;- createDataPartition(preprocessed_data$success, p = 0.8, list = FALSE)\ntrain_data &lt;- preprocessed_data[train_index, ]\ntest_data &lt;- preprocessed_data[-train_index, ]\n\n# Train Random Forest model\nrf_model &lt;- randomForest(\n  success ~ population_density + median_income + competition_count +\n            traffic_score + parking_available,\n  data = train_data,\n  ntree = 500,\n  importance = TRUE\n)\n\nLet’s evaluate how well our model performs:\n\n# Function to evaluate model performance\nevaluate_model &lt;- function(model, test_data) {\n  predictions &lt;- predict(model, test_data)\n  confusion_matrix &lt;- confusionMatrix(predictions, test_data$success)\n\n  # Calculate various metrics\n  accuracy &lt;- confusion_matrix$overall[\"Accuracy\"]\n  precision &lt;- confusion_matrix$byClass[\"Pos Pred Value\"]\n  recall &lt;- confusion_matrix$byClass[\"Sensitivity\"]\n  f1_score &lt;- confusion_matrix$byClass[\"F1\"]\n\n  return(list(\n    accuracy = accuracy,\n    precision = precision,\n    recall = recall,\n    f1_score = f1_score,\n    confusion_matrix = confusion_matrix\n  ))\n}\n\nmodel_evaluation &lt;- evaluate_model(rf_model, test_data)\n\n# Print model performance metrics\ncat(\"Model Performance Metrics:\\n\",\n   sprintf(\"Accuracy: %.3f\\n\", model_evaluation$accuracy),\n   sprintf(\"Precision: %.3f\\n\", model_evaluation$precision), \n   sprintf(\"Recall: %.3f\\n\", model_evaluation$recall),\n   sprintf(\"F1 Score: %.3f\\n\", model_evaluation$f1_score),\n   sep=\"\")\n\nModel Performance Metrics:\nAccuracy: 0.558\nPrecision: 0.417\nRecall: 0.250\nF1 Score: 0.312\n\n\nThe Random Forest model employed in this demonstration serves as a powerful example of how machine learning can process multiple location attributes simultaneously. Through our synthetic dataset, we demonstrate the model’s ability to weigh various factors such as population density, median income, and traffic patterns. The model’s current accuracy of 55.8% in this demonstration context illustrates the basic mechanics of the prediction process, while highlighting the potential for refinement with real-world data.\nTraffic score emerged as the most significant factor (112.36), indicating that high traffic volume strongly correlates with success. Population density (108.07) and median income (107.58) also proved to be strong predictors, suggesting that businesses thrive in densely populated, affluent areas. Competition (40.67), while relevant, appears to be less decisive. Surprisingly, parking availability (12.24) had a considerably lower impact than initially anticipated. These findings offer actionable insights for strategic site selection and underscore the importance of considering multiple factors beyond traditional assumptions"
  },
  {
    "objectID": "posts/Predictive_model/index.html#understanding-feature-importance",
    "href": "posts/Predictive_model/index.html#understanding-feature-importance",
    "title": "Demonstrating Predictive Modeling for Site Selection: A Proof of Concept",
    "section": "Understanding Feature Importance",
    "text": "Understanding Feature Importance\nOne of the most valuable aspects of our model is understanding which factors contribute most to business success:\n\nimportance_scores &lt;- importance(rf_model)\nimportance_df &lt;- data.frame(\n  Feature = rownames(importance_scores),\n  Importance = importance_scores[, \"MeanDecreaseGini\"]\n)\nimportance_df &lt;- importance_df[order(-importance_df$Importance), ]\n\nggplot(importance_df, aes(x = reorder(Feature, Importance), y = Importance)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  coord_flip() +\n  theme_minimal() +\n  labs(\n    title = \"Feature Importance in Site Selection Model\",\n    x = \"Features\",\n    y = \"Importance Score\"\n  )\n\n\n\n\n\n\n\nprint(head(importance_df))\n\n                              Feature Importance\ntraffic_score           traffic_score  112.35753\npopulation_density population_density  108.06737\nmedian_income           median_income  107.58823\ncompetition_count   competition_count   40.66624\nparking_available   parking_available   12.24228\n\n\nOur synthetic model reveals interesting patterns in feature importance, with traffic scores, population density, and median income emerging as the most influential factors. While these results stem from generated data, they showcase how such a model could identify key success drivers in actual market conditions. The visualization of success probabilities across Nigeria’s geography demonstrates the potential for spatial insight in location strategy."
  },
  {
    "objectID": "posts/Predictive_model/index.html#making-predictions-for-new-locations",
    "href": "posts/Predictive_model/index.html#making-predictions-for-new-locations",
    "title": "Demonstrating Predictive Modeling for Site Selection: A Proof of Concept",
    "section": "Making Predictions for New Locations",
    "text": "Making Predictions for New Locations\nNow comes the exciting part – using our model to predict the success probability of new locations:\nNow that we understand our model, let’s apply it to new business locations. We generated 10 new sites across Nigeria and estimated their probability of success using the trained Random Forest model.\n\nset.seed(400)\n# Generate sample points across Nigeria\nn_locations &lt;- 10  \n\n# Define Nigeria's latitude and longitude bounds\nnigeria_lat_range &lt;- c(4.2, 13.9)  # Nigeria's latitudes (south to north)\nnigeria_lon_range &lt;- c(2.7, 14.6)  # Nigeria's longitudes (west to east)\n\n# Generate skewed data\nskewed_population_density &lt;- round(3000 + rgamma(n_locations, shape = 2, scale = 2000))  # Right skewed\nskewed_median_income &lt;- round(40000 + rlnorm(n_locations, meanlog = 10, sdlog = 0.4))  # Log-normal\nskewed_competition_count &lt;- rpois(n_locations, lambda = 3)  # Poisson (counts, skewed)\nskewed_traffic_score &lt;- round(50 + rbeta(n_locations, shape1 = 2, shape2 = 5) * 50)  # Beta skewed toward lower values\n\nnew_sites &lt;- data.frame(\n  location_id = 1001:(1001 + n_locations - 1),\n  latitude = rbeta(n_locations, shape1 = 3, shape2 = 2) * diff(nigeria_lat_range) + nigeria_lat_range[1],  \n  longitude = rbeta(n_locations, shape1 = 2, shape2 = 3) * diff(nigeria_lon_range) + nigeria_lon_range[1],  \n  population_density = skewed_population_density,\n  median_income = skewed_median_income,\n  competition_count = pmin(skewed_competition_count, 10),  # Capping to match original range\n  traffic_score = pmin(skewed_traffic_score, 100),  # Capping at 100\n  parking_available = factor(sample(c(TRUE, FALSE), n_locations, replace = TRUE))\n)\n\n# Verify the data\nprint(head(new_sites))\n\n  location_id  latitude longitude population_density median_income\n1        1001 10.313860  6.352274               3998         57100\n2        1002  9.484589  8.770463               5212         89130\n3        1003 10.070849  9.269369               4561         52912\n4        1004 12.528580  4.308185               4793         56897\n5        1005 12.008947  4.880606               8479         60865\n6        1006 11.902673  9.455585               8024         78579\n  competition_count traffic_score parking_available\n1                 4            60             FALSE\n2                 5            61             FALSE\n3                 2            60             FALSE\n4                 6            63              TRUE\n5                 3            61              TRUE\n6                 5            62             FALSE\n\n# Predict success probabilities\npredictions &lt;- predict(rf_model, new_sites, type = \"prob\")\nnew_sites$success_probability &lt;- predictions[, \"1\"]\n\n# Convert `new_sites` to an sf object\nnew_sites_sf &lt;- st_as_sf(new_sites, coords = c(\"longitude\", \"latitude\"), crs = 4326)\n\n# Ensure points are within Nigeria's land area\nnew_sites_sf &lt;- new_sites_sf[st_within(new_sites_sf, nigeria, sparse = FALSE), ]\n\n\n############ Viz Data #########################\n# Define a diverging color palette (RdYlBu for better contrast)\npal &lt;- colorNumeric(palette = \"RdYlBu\", domain = sample_sf$success_probability * 100, reverse = FALSE)\n\n# Create the Leaflet map with improved color variation\nleaflet(new_sites_sf) |&gt; \n  addProviderTiles(providers$CartoDB.Positron) %&gt;% \n  addCircleMarkers(\n    radius = 4, \n    color = ~pal(success_probability),  # Apply diverging color scale\n    fillOpacity = 0.7,\n    popup = ~paste(\"ID:\", location_id, \n                   \"&lt;br&gt;Population Density:\", round(population_density, 2), \n                   \"&lt;br&gt;Success Probability:\", paste0((success_probability * 100),\"%\"))\n  ) %&gt;% \n  addLegend(\n    pal = pal, \n    values = new_sites_sf$success_probability * 100,  # Fixed multiplication outside of $\n    title = \"Success Probability (%)\",  # Updated legend title for clarity\n    opacity = 1\n  )\n\n\n\n\n\nBased on the model’s insights, the most promising locations tend to exhibit the following characteristics: high traffic scores (above 85), population density exceeding 6,000 people per square kilometer, and median income levels surpassing $70,000. For instance, Site 1005 demonstrates these characteristics and has a predicted success rate of 67.2%, making it a high-potential location. Conversely, Site 1012, with a lower predicted success rate of 31.5%, may be less promising due to factors like lower traffic and income levels"
  },
  {
    "objectID": "posts/Predictive_model/index.html#key-insights-and-business-implications",
    "href": "posts/Predictive_model/index.html#key-insights-and-business-implications",
    "title": "Predictive Modeling with R for Business Site Selection",
    "section": "Key Insights and Business Implications",
    "text": "Key Insights and Business Implications\nThis analysis reveals several important insights:\n\nData-Driven Decision Making: By using machine learning, we can move beyond gut feelings and make decisions based on quantitative evidence.\nFeature Importance: Understanding which factors most strongly influence success allows businesses to prioritize their location criteria.\nPredictive Power: Our model achieves strong predictive performance, demonstrating the value of this analytical approach.\nScalability: This framework can be easily adapted to evaluate multiple potential locations simultaneously."
  },
  {
    "objectID": "posts/Predictive_model/index.html#future-considerations",
    "href": "posts/Predictive_model/index.html#future-considerations",
    "title": "Predictive Modeling with R for Business Site Selection",
    "section": "Future Considerations",
    "text": "Future Considerations\nWhile our model provides valuable insights, there are several ways to enhance this analysis:\n\nIncorporate additional data sources (e.g., foot traffic patterns, social media activity)\nConsider temporal factors (seasonal variations, long-term trends)\nAccount for geographical features and zoning regulations\nInclude demographic trend predictions\n\n\nPractical Applications\nReal-world applications of this predictive modeling approach:\n\nRetail Expansion: Evaluate potential locations for new store openings\n\nRestaurant Chains: Assess the viability of new restaurant locations\n\nService Businesses: Identify promising areas for service-based businesses\n\nReal Estate Development: Analyze potential development sites"
  },
  {
    "objectID": "posts/Predictive_model/index.html#conclusion",
    "href": "posts/Predictive_model/index.html#conclusion",
    "title": "Demonstrating Predictive Modeling for Site Selection: A Proof of Concept",
    "section": "Conclusion",
    "text": "Conclusion\nThis proof of concept demonstrates the potential of machine learning in transforming business location strategy. While built on synthetic data, the demonstrated approach provides a foundation for developing sophisticated, data-driven decision support tools. The future of location strategy lies in combining such analytical capabilities with deep market understanding and local expertise.\nBy combining historical data with machine learning techniques, we can better understand the factors that contribute to business success and make more informed location decisions.\nRemember that while models provide valuable insights, they should complement, not replace, human judgment and domain expertise. The most effective decisions often come from combining quantitative analysis with qualitative understanding of local market dynamics.\n\n\nTechnical Notes\nThe demonstration employs R 4.2.0 along with key packages including tidyverse, caret, randomForest, and sf. This technical stack was chosen to showcase the potential for sophisticated spatial analysis and machine learning in business strategy. The complete code structure provides a template for future development with real-world data."
  },
  {
    "objectID": "posts/Predictive_model/index.html#section",
    "href": "posts/Predictive_model/index.html#section",
    "title": "Predictive Modeling with R for Business Site Selection",
    "section": "",
    "text": "# Create color palette based on success probability\npal &lt;- colorNumeric(\n  palette = \"RdYlGn\",  # Red-Yellow-Green palette\n  domain = new_sites_v1$success_probability\n)\n\n# Create the map\nsw_nigeria_map &lt;- leaflet(new_sites_v1) %&gt;%\n  addProviderTiles(providers$CartoDB.Positron) %&gt;%\n  setView(lng = 4.2, lat = 7.2, zoom = 7.5) %&gt;%  # Centered on SW Nigeria\n  addCircleMarkers(\n    ~longitude, ~latitude,\n    popup = ~paste(\n      \"&lt;strong&gt;ID:\", location_id, \"&lt;/strong&gt;&lt;br&gt;\",\n      \"City:\", c(\"Lagos Island\", \"Ikeja\", \"Ibadan\", \"Abeokuta\", \n                 \"Ogbomoso\", \"Oyo\", \"Oshogbo\", \"Ile-Ife\", \n                 \"Akure\", \"Sagamu\"), \"&lt;br&gt;\",\n      \"Success Probability:\", sprintf(\"%.1f%%\", success_probability * 100), \"&lt;br&gt;\",\n      \"Population Density:\", population_density, \"&lt;br&gt;\",\n      \"Median Income:\", median_income, \"&lt;br&gt;\",\n      \"Competition Count:\", competition_count, \"&lt;br&gt;\",\n      \"Traffic Score:\", traffic_score\n    ),\n    label = ~sprintf(\"ID: %d (%.1f%%)\", location_id, success_probability * 100),\n    radius = 8,\n    color = ~pal(success_probability),\n    fillOpacity = 0.7,\n    stroke = TRUE,\n    weight = 2\n  ) %&gt;%\n  addLegend(\n    position = \"bottomright\",\n    pal = pal,\n    values = ~success_probability,\n    title = \"Success Probability\",\n    opacity = 0.7,\n    labFormat = labelFormat(suffix = \"%\", transform = function(x) 100 * x)\n  )\n\n# Display the map\nsw_nigeria_map"
  },
  {
    "objectID": "posts/Predictive_model/index.html#new-data",
    "href": "posts/Predictive_model/index.html#new-data",
    "title": "Predictive Modeling with R for Business Site Selection",
    "section": "New Data",
    "text": "New Data\n\nset.seed(400)\n# Generate sample points across Nigeria\nn_locations &lt;- 10  \n\n# Define Nigeria's latitude and longitude bounds\nnigeria_lat_range &lt;- c(4.2, 13.9)  # Nigeria's latitudes (south to north)\nnigeria_lon_range &lt;- c(2.7, 14.6)  # Nigeria's longitudes (west to east)\n\n# Generate skewed data\nskewed_population_density &lt;- round(3000 + rgamma(n_locations, shape = 2, scale = 2000))  # Right skewed\nskewed_median_income &lt;- round(40000 + rlnorm(n_locations, meanlog = 10, sdlog = 0.4))  # Log-normal\nskewed_competition_count &lt;- rpois(n_locations, lambda = 3)  # Poisson (counts, skewed)\nskewed_traffic_score &lt;- round(50 + rbeta(n_locations, shape1 = 2, shape2 = 5) * 50)  # Beta skewed toward lower values\n\nnew_sites &lt;- data.frame(\n  location_id = 1001:(1001 + n_locations - 1),\n  latitude = rbeta(n_locations, shape1 = 3, shape2 = 2) * diff(nigeria_lat_range) + nigeria_lat_range[1],  \n  longitude = rbeta(n_locations, shape1 = 2, shape2 = 3) * diff(nigeria_lon_range) + nigeria_lon_range[1],  \n  population_density = skewed_population_density,\n  median_income = skewed_median_income,\n  competition_count = pmin(skewed_competition_count, 10),  # Capping to match original range\n  traffic_score = pmin(skewed_traffic_score, 100),  # Capping at 100\n  parking_available = factor(sample(c(TRUE, FALSE), n_locations, replace = TRUE))\n)\n\n# Verify the data\nprint(head(new_sites))\n\n  location_id  latitude longitude population_density median_income\n1        1001 10.313860  6.352274               3998         57100\n2        1002  9.484589  8.770463               5212         89130\n3        1003 10.070849  9.269369               4561         52912\n4        1004 12.528580  4.308185               4793         56897\n5        1005 12.008947  4.880606               8479         60865\n6        1006 11.902673  9.455585               8024         78579\n  competition_count traffic_score parking_available\n1                 4            60             FALSE\n2                 5            61             FALSE\n3                 2            60             FALSE\n4                 6            63              TRUE\n5                 3            61              TRUE\n6                 5            62             FALSE\n\n# Predict success probabilities\npredictions &lt;- predict(rf_model, new_sites, type = \"prob\")\nnew_sites$success_probability &lt;- predictions[, \"1\"]\n\n# Convert `new_sites` to an sf object\nnew_sites_sf &lt;- st_as_sf(new_sites, coords = c(\"longitude\", \"latitude\"), crs = 4326)\n\n# Ensure points are within Nigeria's land area\nnew_sites_sf &lt;- new_sites_sf[st_within(new_sites_sf, nigeria, sparse = FALSE), ]\n\n\n############ Viz Data #########################\n# Define a diverging color palette (RdYlBu for better contrast)\npal &lt;- colorNumeric(palette = \"RdYlBu\", domain = sample_sf$success_probability * 100, reverse = FALSE)\n\n# Create the Leaflet map with improved color variation\nleaflet(new_sites_sf) |&gt; \n  addProviderTiles(providers$CartoDB.Positron) %&gt;% \n  addCircleMarkers(\n    radius = 4, \n    color = ~pal(success_probability),  # Apply diverging color scale\n    fillOpacity = 0.7,\n    popup = ~paste(\"ID:\", location_id, \n                   \"&lt;br&gt;Population Density:\", round(population_density, 2), \n                   \"&lt;br&gt;Success Probability:\", paste0((success_probability * 100),\"%\"))\n  ) %&gt;% \n  addLegend(\n    pal = pal, \n    values = new_sites_sf$success_probability * 100,  # Fixed multiplication outside of $\n    title = \"Success Probability (%)\",  # Updated legend title for clarity\n    opacity = 1\n  )"
  },
  {
    "objectID": "posts/Predictive_model/index.html#key-insights-business-implications-and-considerations",
    "href": "posts/Predictive_model/index.html#key-insights-business-implications-and-considerations",
    "title": "Demonstrating Predictive Modeling for Site Selection: A Proof of Concept",
    "section": "Key Insights, Business Implications and Considerations",
    "text": "Key Insights, Business Implications and Considerations\nBy using machine learning, we can move beyond gut feelings and make decisions based on quantitative evidence . These findings suggest focusing location search efforts on high-traffic areas in densely populated, affluent regions, even if they come at a premium. The data indicates this approach is more likely to succeed than choosing less expensive locations with weaker market fundamentals. This framework can be easily adapted to evaluate multiple potential locations simultaneously\nWhile our model provides valuable insights, there are several ways to enhance this analysis. There is a need to incorporate additional data sources (e.g., foot traffic patterns, social media activity) with considerations for industry indicators or features. More so, c Consider temporal factors (seasonal variations, long-term trends) and account for geographical features and zoning regulations.\n\nPractical Applications\nReal-world applications of this predictive modeling approach:\n\nRetail Expansion: Evaluate potential locations for new store openings\n\nRestaurant Chains: Assess the viability of new restaurant locations\n\nService Businesses: Identify promising areas for service-based businesses\n\nReal Estate Development: Analyze potential development sites"
  }
]